# -*- coding: utf-8 -*-
"""fine tuning t5 model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ue1AN7VCXIVo8muZsqtj5k4VaAIg6vE-

**Installation**
"""

pip install happytransformer

"""**Model**


we are using T5 model,T5 comes in several different sizes, and we'll use the base model, which has 220 million parameters. T5 is a text-to-text model, meaning given text, it generated a standalone piece of text based on the input.
"""

from happytransformer import HappyTextToText

happy_tt = HappyTextToText("T5", "t5-base")

"""**Data Collection**

We'll use a well-known dataset called JFLEG to train the model. According to its description on its Hugging Face page, it is "a gold standard benchmark for developing and evaluating GEC systems with respect to fluency" (GEC stands for grammar error correction.) In addition, its paper currently has 106 citations, according to Google Scholar, which shows that it is indeed respected within the NLP community .
"""

pip install dataset

from datasets import load_dataset

"""The dataset's id is "jfleg" and has two splits â€Œ"validation" and "test." We'll the validation set for training and the test set for evaluation."""

train_dataset = load_dataset("jfleg", split='validation[:]')
eval_dataset = load_dataset("jfleg", split='test[:]')

"""**Data Examination**"""

for case in train_dataset["corrections"][:2]:
  print(case)
  print(case[0])
  print("--------------------------------------------------------")

"""**Data Preprocessing**

we must process the into the proper format for Happy Transformer.We need to structure both of the training and evaluating data into the same format, which is a CSV file with two columns: input and target. The input column contains grammatically incorrect text, and the target column contains text that is the corrected version of the text from the target column.

Below is code that processes data into the proper format. We must specify the task we wish to perform by adding the same prefix to each input. In this case, we'll use the prefix "grammar: ". This is done because T5 models are able to perform multiple tasks like translation and summarization with a single model, and a unique prefix is used for each task so that the model learns which task to perform. We also need to skip over cases that contain a blank string to avoid errors while fine-tuning.
"""

import csv

def generate_csv(csv_path, dataset):
    with open(csv_path, 'w', newline='') as csvfile:
        writter = csv.writer(csvfile)
        writter.writerow(["input", "target"])
        for case in dataset:
     	    # Adding the task's prefix to input
            input_text = "grammar: " + case["sentence"]
            for correction in case["corrections"]:
                # a few of the cases contain blank strings.
                if input_text and correction:
                    writter.writerow([input_text, correction])



generate_csv("train.csv", train_dataset)
generate_csv("eval.csv", eval_dataset)



"""**Before Training Evaluating**

We'll evaluate the model before and after fine-tuning using a common metric called loss.

Loss can be described as how "wrong" the model's predictions are compared to the correct answers.

So, if the loss decreases after fine-tuning, then that suggests the model learned.

It's important that we use separate data for training and evaluating to show that the model can generalize its obtained knowledge to solve unseen cases.
"""

before_result = happy_tt.eval("eval.csv")

print("Before loss:", before_result.loss)

"""**Training**

Let's now train the model.

We can do so by calling happy_tt's train() method.
"""

from happytransformer import TTTrainArgs
args = TTTrainArgs(batch_size=8)
happy_tt.train("train.csv", args=args)

"""**After Training Evaluating**

let's determine the model's loss.
"""

before_loss = happy_tt.eval("eval.csv")
print("After loss: ", before_loss.loss)

"""**Inference**


Let's now use the model to correct the grammar of examples we'll provide it. To accomplish this, we'll use happy_tt's generate_text() method. We'll also use an algorithm called beam search for the generation.
"""

from happytransformer import TTSettings

beam_settings =  TTSettings(num_beams=5, min_length=1, max_length=20)

"""**sample sentences with correction**

1)Incorrect: "He don't like pizza."

Corrected: "He doesn't like pizza."


2)Incorrect: "She have two cats."

Corrected: "She has two cats."


3)Incorrect: "I can plays the guitar."

Corrected: "I can play the guitar."


4)Incorrect: "They was at the party last night."

Corrected: "They were at the party last night."


5)Incorrect: "She writed a beautiful poem."

Corrected: "She wrote a beautiful poem."


6)Incorrect: "The cat is laying on the couch."

Corrected: "The cat is lying on the couch."


7)Incorrect: "He have been to Paris before."

Corrected: "He has been to Paris before."


8)Incorrect: "The book is laying on the table."

Corrected: "The book is lying on the table."


9)Incorrect: "She don't have any siblings."

Corrected: "She doesn't have any siblings."


10)Incorrect: "We can goes to the park tomorrow."

Corrected: "We can go to the park tomorrow."
"""

example_1 = "He don't like pizza."
result_1 = happy_tt.generate_text(example_1, args=beam_settings)
print(result_1.text)

example_2 = "She have two cats."

result_2 = happy_tt.generate_text(example_2, args=beam_settings)
print(result_2.text)

example_3 = "im good fr nothing."

result_3 = happy_tt.generate_text(example_3, args=beam_settings)
print(result_3.text)

example_3 = "I can plays the guitar."
result_3 = happy_tt.generate_text(example_3, args=beam_settings)
print("Example 3:", result_3.text)

example_4 = "They was at the party last night."
result_4 = happy_tt.generate_text(example_4, args=beam_settings)
print("Example 4:", result_4.text)

example_5 = "She writed a beautiful poem."
result_5 = happy_tt.generate_text(example_5, args=beam_settings)
print("Example 5:", result_5.text)

example_6 = "The cat is laying on the couch."
result_6 = happy_tt.generate_text(example_6, args=beam_settings)
print("Example 6:", result_6.text)

example_7 = "He have been to Paris before."
result_7 = happy_tt.generate_text(example_7, args=beam_settings)
print("Example 7:", result_7.text)

example_8 = "The book is laying on the table."
result_8 = happy_tt.generate_text(example_8, args=beam_settings)
print("Example 8:", result_8.text)

example_9 = "She don't have any siblings."
result_9 = happy_tt.generate_text(example_9, args=beam_settings)
print("Example 9:", result_9.text)

example_10 = "We can goes to the park tomorrow."
result_10 = happy_tt.generate_text(example_10, args=beam_settings)
print("Example 10:", result_10.text)

"""**Further Improvement:**
I suggest transferring some of the evaluating cases to the training data and then optimize the hyperparameters by applying a technique like grid search.

You can then include the evaluating cases in the training set to fine-tune a final model using your best set of hyperparameters.

Even we can try multiple languages to support multilinguality.

Add custom layers to refine output.

Try other models as well.
"""

